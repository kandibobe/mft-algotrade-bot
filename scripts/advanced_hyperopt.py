#!/usr/bin/env python3
"""
Advanced Hyperparameter Optimization for XGBoost Trading Models
================================================================

Robust hyperparameter optimization focusing on Sharpe Ratio optimization
with strict constraints to prevent overfitting.

CRITICAL CONSTRAINTS:
1. Search Space:
   - max_depth: Integer [2, 3, 4] (Strictly shallow trees)
   - learning_rate: Float [0.001, 0.05] (Log scale)
   - n_estimators: Integer [100, 1000] (With Early Stopping enabled)
   - gamma: Float [0.1, 5.0] (High regularization)
   - subsample: Float [0.5, 0.8]
   - reg_alpha (L1): Float [1.0, 10.0]

2. Objective Function:
   - Optimize for Sharpe Ratio (Risk-Adjusted Return) on Validation set
   - Calculate PnL of trades generated by the model -> Calculate Sharpe

3. Cross-Validation:
   - Use TimeSeriesSplit (5 folds). Do not shuffle data.
   - Metric must be the average Sharpe Ratio across all 5 folds.

4. Pruning:
   - Use optuna.pruners.MedianPruner. If a trial looks bad after fold 1, kill it.

OUTPUT:
- Saves best params to user_data/model_best_params.json
- n_trials=200

Author: Stoic Citadel Team
Date: December 23, 2025
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import pandas as pd
import optuna
from optuna.trial import Trial
from optuna.pruners import MedianPruner
from sklearn.model_selection import TimeSeriesSplit
import xgboost as xgb

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.data.loader import get_ohlcv
from src.ml.training.feature_engineering import FeatureEngineer
from src.ml.training.labeling import TripleBarrierLabeler, TripleBarrierConfig
from src.utils.risk import calculate_sharpe_ratio

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SharpeRatioObjective:
    """
    Objective function for Optuna that optimizes Sharpe Ratio.
    
    Calculates PnL from model predictions and computes Sharpe Ratio
    for risk-adjusted performance evaluation.
    """
    
    def __init__(self, X: pd.DataFrame, y: pd.Series, n_splits: int = 5):
        """
        Initialize objective function.
        
        Args:
            X: Feature matrix
            y: Target labels (1 for LONG, -1 for SHORT, 0 for NEUTRAL)
            n_splits: Number of time series splits
        """
        self.X = X
        self.y = y
        self.n_splits = n_splits
        
        # Store prices for PnL calculation (assuming close prices are in features)
        # We'll extract close prices from feature names if available
        self.close_prices = self._extract_close_prices(X)
        
    def _extract_close_prices(self, X: pd.DataFrame) -> Optional[pd.Series]:
        """
        Extract close prices from features if available.
        
        In the feature engineering pipeline, close prices might be included
        as a feature or we can reconstruct from the original data.
        For simplicity, we'll look for columns containing 'close'.
        """
        close_cols = [col for col in X.columns if 'close' in col.lower()]
        if close_cols:
            # Use the first close column found
            return X[close_cols[0]]
        return None
    
    def calculate_pnl(self, y_true: pd.Series, y_pred: pd.Series, 
                     close_prices: pd.Series, 
                     hold_period: int = 1) -> pd.Series:
        """
        Calculate PnL from trading signals.
        
        Args:
            y_true: True labels (not used for PnL, only for validation)
            y_pred: Predicted labels (0: SHORT, 1: NEUTRAL, 2: LONG) - mapped from [-1, 0, 1]
            close_prices: Close prices for PnL calculation
            hold_period: Number of periods to hold position
            
        Returns:
            Series of returns
        """
        # Align indices
        common_idx = y_pred.index.intersection(close_prices.index)
        y_pred = y_pred.loc[common_idx]
        close_prices = close_prices.loc[common_idx]
        
        # Calculate returns
        future_returns = close_prices.pct_change(hold_period).shift(-hold_period)
        
        # Calculate PnL: LONG -> future return, SHORT -> -future return, NEUTRAL -> 0
        pnl = pd.Series(0.0, index=y_pred.index)
        
        # LONG positions (mapped from 2)
        long_mask = y_pred == 2
        if long_mask.any():
            pnl[long_mask] = future_returns[long_mask]
        
        # SHORT positions (mapped from 0)  
        short_mask = y_pred == 0
        if short_mask.any():
            pnl[short_mask] = -future_returns[short_mask]
        
        # NEUTRAL positions (mapped from 1) remain 0
        
        return pnl.dropna()
    
    def __call__(self, trial: Trial) -> float:
        """
        Objective function for Optuna.
        
        Args:
            trial: Optuna trial
            
        Returns:
            Sharpe Ratio (to be maximized)
        """
        # Suggest hyperparameters with strict constraints
        params = {
            'max_depth': trial.suggest_int('max_depth', 2, 4),  # Shallow trees only
            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.05, log=True),
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'gamma': trial.suggest_float('gamma', 0.1, 5.0),  # High regularization
            'subsample': trial.suggest_float('subsample', 0.5, 0.8),
            'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 10.0),  # L1 regularization
            'reg_lambda': 1.0,  # Fixed L2
            'min_child_weight': 5,  # Fixed for simplicity
            'colsample_bytree': 0.7,  # Fixed
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': 0,
            'use_label_encoder': False,
            'eval_metric': 'mlogloss',  # Multi-class log loss
            'objective': 'multi:softmax',  # Multi-class classification
            'num_class': 3  # 3 classes: SHORT (0), NEUTRAL (1), LONG (2)
        }
        
        # Time series cross-validation
        tscv = TimeSeriesSplit(n_splits=self.n_splits)
        sharpe_ratios = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(self.X)):
            # Split data
            X_train, X_val = self.X.iloc[train_idx], self.X.iloc[val_idx]
            y_train, y_val = self.y.iloc[train_idx], self.y.iloc[val_idx]
            
            # Create and train model with early stopping
            model_params = params.copy()
            model_params['early_stopping_rounds'] = 20
            model = xgb.XGBClassifier(**model_params)
            
            # Train with validation set for early stopping
            eval_set = [(X_val, y_val)]
            model.fit(
                X_train, y_train,
                eval_set=eval_set,
                verbose=False
            )
            
            # Predict on validation set
            y_pred = model.predict(X_val)
            # Convert to Series with same index as X_val for alignment
            y_pred_series = pd.Series(y_pred, index=X_val.index)
            
            # Calculate PnL
            if self.close_prices is not None:
                val_close_prices = self.close_prices.iloc[val_idx]
                pnl = self.calculate_pnl(y_val, y_pred_series, val_close_prices)
                
                if len(pnl) > 1:
                    sharpe = calculate_sharpe_ratio(pnl)
                    sharpe_ratios.append(sharpe)
                else:
                    sharpe_ratios.append(0.0)
            else:
                # Fallback to accuracy if no close prices
                accuracy = (y_pred == y_val).mean()
                sharpe_ratios.append(accuracy - 0.5)  # Convert to [-0.5, 0.5] range
            
            # Pruning after first fold
            if fold == 0:
                trial.report(sharpe_ratios[-1], fold)
                if trial.should_prune():
                    raise optuna.TrialPruned()
        
        # Return average Sharpe Ratio across folds
        avg_sharpe = np.mean(sharpe_ratios) if sharpe_ratios else 0.0
        return avg_sharpe


class AdvancedHyperparameterOptimizer:
    """
    Advanced hyperparameter optimizer for XGBoost trading models.
    
    Focuses on Sharpe Ratio optimization with strict regularization
    to prevent overfitting.
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize optimizer.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config or {}
        self.study = None
        self.best_params = None
        self.best_value = None
        self.trial_results = []
        
        # Create output directory
        self.output_dir = Path("user_data")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def load_and_prepare_data(self, 
                             symbol: str = "BTC/USDT",
                             timeframe: str = "5m",
                             start: Optional[str] = None,
                             end: Optional[str] = None) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Load and prepare data for optimization.
        
        Args:
            symbol: Trading pair
            timeframe: Candle timeframe
            start: Start date
            end: End date
            
        Returns:
            Tuple of (X, y) - features and labels
        """
        logger.info(f"Loading data for {symbol} {timeframe}")
        
        # Load OHLCV data
        df = get_ohlcv(
            symbol=symbol,
            timeframe=timeframe,
            start=start,
            end=end,
            exchange="binance",
            use_cache=True
        )
        
        logger.info(f"Loaded {len(df)} candles from {df.index[0]} to {df.index[-1]}")
        
        # Feature engineering
        logger.info("Generating features...")
        feature_engineer = FeatureEngineer()
        X = feature_engineer.fit_transform(df)
        
        # Labeling
        logger.info("Generating labels...")
        labeler = TripleBarrierLabeler(config=TripleBarrierConfig())
        y = labeler.label(df)
        
        # Align features and labels
        common_index = X.index.intersection(y.index)
        X = X.loc[common_index]
        y = y.loc[common_index]
        
        # Remove rows with NaN labels
        nan_mask = y.isna()
        if nan_mask.any():
            logger.info(f"Removing {nan_mask.sum()} rows with NaN labels")
            X = X[~nan_mask]
            y = y[~nan_mask]
        
        # Convert labels from [-1, 0, 1] to [0, 1, 2] for XGBoost multi-class classification
        # XGBClassifier expects labels starting from 0
        y = y.map({-1: 0, 0: 1, 1: 2})
        
        logger.info(f"Final dataset: {len(X)} samples, {X.shape[1]} features")
        logger.info(f"Label distribution (mapped): {y.value_counts().to_dict()}")
        
        return X, y
    
    def optimize(self, 
                 X: pd.DataFrame, 
                 y: pd.Series,
                 n_trials: int = 200,
                 n_splits: int = 5,
                 timeout: Optional[int] = None) -> Dict[str, Any]:
        """
        Run hyperparameter optimization.
        
        Args:
            X: Features
            y: Labels
            n_trials: Number of Optuna trials
            n_splits: Number of time series splits
            timeout: Optimization timeout in seconds
            
        Returns:
            Dictionary with optimization results
        """
        logger.info(f"Starting hyperparameter optimization with {n_trials} trials")
        
        # Create objective function
        objective = SharpeRatioObjective(X, y, n_splits=n_splits)
        
        # Create study with MedianPruner
        self.study = optuna.create_study(
            direction="maximize",
            study_name="xgboost_sharpe_optimization",
            pruner=MedianPruner(
                n_startup_trials=5,
                n_warmup_steps=1,
                interval_steps=1
            )
        )
        
        # Run optimization
        self.study.optimize(
            objective,
            n_trials=n_trials,
            timeout=timeout,
            show_progress_bar=True,
            n_jobs=1  # Use 1 job for stability with time series
        )
        
        # Get best results
        self.best_params = self.study.best_params
        self.best_value = self.study.best_value
        
        logger.info(f"Optimization completed. Best Sharpe Ratio: {self.best_value:.4f}")
        logger.info(f"Best parameters: {self.best_params}")
        
        # Train final model with best parameters
        final_model = self._train_final_model(X, y, self.best_params)
        
        return {
            "best_params": self.best_params,
            "best_value": self.best_value,
            "study": self.study,
            "final_model": final_model,
            "timestamp": datetime.now().isoformat()
        }
    
    def _train_final_model(self, X: pd.DataFrame, y: pd.Series, 
                          params: Dict) -> xgb.XGBClassifier:
        """
        Train final model with best parameters on full dataset.
        
        Args:
            X: Features
            y: Labels
            params: Best hyperparameters
            
        Returns:
            Trained XGBoost model
        """
        logger.info("Training final model with best parameters...")
        
        # Prepare parameters for final training
        final_params = params.copy()
        final_params['n_estimators'] = 1000  # Use more trees for final model
        final_params['early_stopping_rounds'] = None  # No early stopping for final
        
        # Train on full dataset
        model = xgb.XGBClassifier(**final_params)
        model.fit(X, y)
        
        logger.info("Final model trained successfully")
        return model
    
    def save_results(self, results: Dict[str, Any]):
        """
        Save optimization results to disk.
        
        Args:
            results: Optimization results
        """
        # Save best parameters
        params_path = self.output_dir / "model_best_params.json"
        with open(params_path, 'w') as f:
            json.dump(results["best_params"], f, indent=2)
        logger.info(f"Best parameters saved to {params_path}")
        
        # Save full results
        results_path = self.output_dir / "hyperopt_results.json"
        
        # Convert study to serializable format
        serializable_results = {
            "best_params": results["best_params"],
            "best_value": results["best_value"],
            "timestamp": results["timestamp"],
            "n_trials_completed": len(results["study"].trials),
            "trials_summary": [
                {
                    "number": t.number,
                    "value": t.value,
                    "params": t.params,
                    "state": str(t.state)
                }
                for t in results["study"].trials
            ]
        }
        
        with open(results_path, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        logger.info(f"Full results saved to {results_path}")
        
        # Save importance plot if possible
        try:
            self._plot_importance(results["final_model"])
        except Exception as e:
            logger.warning(f"Could not create importance plot: {e}")
    
    def _plot_importance(self, model: xgb.XGBClassifier):
        """
        Plot and save feature importance.
        
        Args:
            model: Trained XGBoost model
        """
        import matplotlib.pyplot as plt
        
        # Get feature importance
        importance = model.feature_importances_
        feature_names = model.get_booster().feature_names
        
        if feature_names is None:
            logger.warning("No feature names available for importance plot")
            return
        
        # Create DataFrame
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False).head(20)
        
        # Plot
        plt.figure(figsize=(10, 6))
        plt.barh(range(len(importance_df)), importance_df['importance'])
        plt.yticks(range(len(importance_df)), importance_df['feature'])
        plt.xlabel('Importance')
        plt.title('Top 20 Feature Importance')
        plt.tight_layout()
        
        # Save
        plot_path = self.output_dir / "feature_importance.png"
        plt.savefig(plot_path, dpi=150)
        plt.close()
        
        logger.info(f"Feature importance plot saved to {plot_path}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Advanced hyperparameter optimization for XGBoost trading models"
    )
    
    parser.add_argument(
        '--symbol',
        type=str,
        default='BTC/USDT',
        help='Trading pair (default: BTC/USDT)'
    )
    parser.add_argument(
        '--timeframe',
        type=str,
        default='5m',
        help='Timeframe (default: 5m)'
    )
    parser.add_argument(
        '--start',
        type=str,
        default=None,
        help='Start date (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--end',
        type=str,
        default=None,
        help='End date (YYYY-MM-DD)'
    )
    parser.add_argument(
        '--trials',
        type=int,
        default=200,
        help='Number of Optuna trials (default: 200)'
    )
    parser.add_argument(
        '--splits',
        type=int,
        default=5,
        help='Number of time series splits (default: 5)'
    )
    parser.add_argument(
        '--timeout',
        type=int,
        default=None,
        help='Optimization timeout in seconds (default: None)'
    )
    parser.add_argument(
        '--quick',
        action='store_true',
        help='Quick mode (use only recent data for testing)'
    )
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("üöÄ ADVANCED HYPERPARAMETER OPTIMIZATION")
    print("="*70)
    print(f"\nConfiguration:")
    print(f"  Symbol:     {args.symbol}")
    print(f"  Timeframe:  {args.timeframe}")
    print(f"  Start:      {args.start}")
    print(f"  End:        {args.end}")
    print(f"  Trials:     {args.trials}")
    print(f"  Splits:     {args.splits}")
    print(f"  Timeout:    {args.timeout}")
    print(f"  Quick mode: {args.quick}")
    print("\n" + "="*70)
    
    try:
        # Initialize optimizer
        optimizer = AdvancedHyperparameterOptimizer()
        
        # Load and prepare data
        X, y = optimizer.load_and_prepare_data(
            symbol=args.symbol,
            timeframe=args.timeframe,
            start=args.start,
            end=args.end
        )
        
        # For quick mode, use only recent data
        if args.quick:
            print("\n‚ö° Quick mode: Using only 1000 most recent samples")
            X = X.tail(1000)
            y = y.tail(1000)
            args.trials = min(args.trials, 20)  # Reduce trials for quick mode
        
        # Run optimization
        results = optimizer.optimize(
            X=X,
            y=y,
            n_trials=args.trials,
            n_splits=args.splits,
            timeout=args.timeout
        )
        
        # Save results
        optimizer.save_results(results)
        
        print("\n" + "="*70)
        print("‚úÖ OPTIMIZATION COMPLETED SUCCESSFULLY!")
        print("="*70)
        print(f"\nBest Sharpe Ratio: {results['best_value']:.4f}")
        print(f"\nBest parameters saved to: user_data/model_best_params.json")
        print(f"Full results saved to: user_data/hyperopt_results.json")
        print("\n" + "="*70)
        
    except Exception as e:
        print(f"\n‚ùå Optimization failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
